<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Computer Vision Course Project
        | ECE, Virginia Tech | Fall 2024: ECE 4554/5554</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Vision based Assistive Navigation</h1>
        <span style="font-size: 18px; line-height: 1.5em;"><strong>Clifford Reeve Menezes, Pratheek Prakash Shetty</strong></span><br>
        <span style="font-size: 16px; line-height: 1.5em;">Fall 2024 ECE 4554/5554 Computer Vision: Course Project</span><br>
        <span style="font-size: 16px; line-height: 1.5em;">Virginia Tech</span>
        <p>This project uses object detection and image captioning models to provide hazard avoidance directions for users with visual impairments.</p>
    </header>

    <section id="abstract">
        <h2>Abstract</h2>
        <p>For visually impaired people, traveling alone in the modern world can be difficult, especially when crossing roadways or hazards in real-time. Our idea is to create safe smartphone navigation, eliminating the need for further special equipment while guaranteeing smooth guidance. To solve this problem, we created an assistive navigation system that uses YOLO for object identification, BLIP for scene captioning, and an API for a generative AI model that provides precise descriptions and guidance of hazards in their path in real-time. Our model achieves a —--- success rate with —----something.
        </p>
        
    </section>
    <section id="teaser">
        <h2>Teaser figure</h2>
        Assisting Navigation and Hazards using vision on the smartphone
        <br><br>
        <div style="text-align: center;">
            <img style="height: 200px;" alt="" src="teaser.png">
        </div>
        <p style="text-align: center;">*This is not an image from the actual model*</p>
    </section>

    <section id = "introduction">
        <h2>Introduction</h2>
        <p>What if someone who is visually challenged could "see" what is going on around them? For the visually impaired, navigating the world may be a challenging task. While current solutions such as wearable technology, guide dogs, and specialized walking aids provide invaluable support, their accessibility, affordability, and efficacy are sometimes limited. On the other hand, depending just on a mobile phone offers a simpler and more economical method, taking advantage of smartphones' popularity.</p>

        <p>We are using a pre-trained YOLO model, which can perform real-time object detection with high accuracy. The system can efficiently identify hazards and obstacles such as vehicles, staircases, and pedestrians. When objects are detected, the results are sent to a BLIP model, which processes the scene and generates descriptive captions. These captions are then passed on to a generative AI tool via an API call. The AI predicts the most efficient course of action for the user. The resulting directions are converted into speech, which guides the user safely along the path. The approach combines fields in object detection, scene captioning, and generative AI.</p>

        <p>Compared to traditional methods, this system stands out for its use of a widely available mobile device, which reduces the need for specialized tools. Additionally, the system offers a more adaptive and user-centric solution by incorporating generative AI to refine the directions.
        </p>

        <!-- add an image that shows workflow here or in the appraoch section -->

    </section>

    <section id = "approach">
        <h2>Approach</h2>
        <p>In order to be able to provide the required real-time hazard detection and guidance to visually impaired pedestrians, this project is going to rely on sophisticated Computer Vision and natural language processing techniques.</p>
        <p>To approach the problem, we broke it down into smaller tasks.</p>
        <ol>
            <li>Find an efficient way to detect objects in real time.</li>
            <li>Caption the scene with high accuracy.</li>
            <li>Utilize an LLM that can provide the best results for the scene in front of the user.</li>
            <li>Convert the instructions into audio-based instruction.</li>
        </ol>
        <p>Initially, we reviewed papers that could give us insight into the different methods and ideas used to solve the problem. We realized that no paper considered using a mobile device for object detection in this area, which inspired us to use the idea.</p>

        <!-- (mention some papers here or make a related works page) -->

        <p>The closest approach to our paper is [], which used 3D-printed glasses to detect objects using IoT.</p> 
        <!-- mention that paper -->

        <p><b>Task 1: Finding an object detection model</b></p>
        <p>After reviewing the papers, we realized that YOLO models are efficient in object recognition and can be easily trained. However, the YOLOv8 nano is a pre-trained model that can easily detect and classify objects in real time. We tested this model by sending it test videos of Blacksburg, where it quickly identified common hazards such as cars, pedestrians, and even benches.</p>
        <p>One problem we faced here is the ethical problem of having a pedestrian being recorded. We had to request permission from pedestrians to use the record data. Therefore, we could not test with multiple videos that involved pedestrians. However, even with limited testing, the model could accurately identify objects.</p>

        <p><b>Task 2: Captioning the objects with high accuracy.</b></p>
        <p>In the image or scene captioning world, many models accurately caption images in real-time. We experimented with the BLIP model, which captioned the objects sent by YOLO with almost complete accuracy. An observation noticed here was that sometimes the captions generated in an indoor environment were not as accurate compared to an outdoor environment. However, our model solely focuses on an outdoor environment, so this did not affect us.</p>

        <p><b>Task 3: Utilize an LLM that can provide the best results for the scene in front of the user.</b></p>
        <p>Our initial approach was to use a local transformer like TinyBert LLM, which can locally give directions. However, upon implementation, we realized that the model gave incorrect results and required a lot of training, which could take a lot of computational power. Overall, implementing such a model locally could potentially risk draining the phone's battery or overheating the device. We eventually decided to make an API call to a preexisting LLM online. We gave a prompt to this model so it could identify solutions with ease. Upon testing with this LLM using live footage, we received accurate results in real time.</p>

        <p><b>Convert the instructions into audio-based instruction.</b></p>
        <!-- add this part -->
    </section>
    

    <section id="workflow">
        <h2>Workflow</h2>
        <ol>
            <li>Capture video from a phone's camera.</li>
            <li>Process frames using YOLO or MobileNet SSD to detect objects.</li>
            <li>Use BLIP to generate captions describing detected objects.</li>
            <li>Assisting Directions: We will make use of TinyBERT LLM, specially optimized to generate helpful directions with a minimum amount of information for real time communication with the user.</li>
            <li> Dataset: Our dataset will consist of annotated images relevant for pedestrian navigation-objects and environmental hazards that may pose a danger to people commuting on foot.</li>
        </ol>
    </section>

    <section id="success">
        <h2>Sucess Parameters</h2>
        <p>The project would be considered a success if the navigation system can efficiently identify hazardous objects in Real-Time.</p>
    </section>

    <section id="exipirimental">
        <h2>Data Collection</h2>
        <p>We shall utilize a comprehensive dataset, with various images of urban scenes, crosswalks, sidewalks, and any sign and obstacle or pedestrians, vehicles, and other obstacles that might be detrimental, like potholes. This dataset should represent environmental conditions from day to night, including weather conditions.</p>
    </section>

    <!-- <section id="demo">
        <h2>Demo Video</h2>
        <p>Check out a demo video of the system in action:</p>
        <video controls>
            <source src="demo.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </section> -->

    <footer>
        <p> © Project by Clifford Reeve Menezes and Pratheek Prakash Shetty group 39</p>
        <a href="https://github.com/gosLp/cv-assistive-nav" target="_blank">GitHub Repository</a>
    </footer>
</body>
</html>
