<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Computer Vision Course Project
        | ECE, Virginia Tech | Fall 2024: ECE 4554/5554</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <nav class="navbar">
            <div class="container">
                <a href="#abstract">Abstract</a>
                <a href="#teaser">Teaser</a>
                <a href="#introduction">Introduction</a>
                <a href="#approach">Approach</a>
                <a href="#Experiments">Experiments</a>
                <a href="#qualitative-comparison">Qualitative Results</a>
                <a href="#conclusion">Conclusion</a>
                <a href="#references">References</a>
            </div>
        </nav>
        <h1>Vision based Assistive Navigation</h1>
        <div class="subtitle">
            <strong>Clifford Reeve Menezes, Pratheek Prakash Shetty</strong><br>
            Fall 2024 ECE 4554/5554 Computer Vision: Course Project<br>
            Virginia Tech
        </div>
        <p class="header-description">
            This project uses object detection and image captioning models to provide hazard avoidance directions for users with visual impairments.
        </p>
    </header>
    <main>


        <section id="abstract">
            <h2>Abstract</h2>
            <p>For visually impaired people, traveling alone in the modern world can be difficult, especially when crossing roadways or hazards in real-time. Our idea is to create safe smartphone navigation, eliminating the need for further special equipment while guaranteeing smooth guidance. Our aim us to help people having such problems and support them in their day-to-day activities. To solve this problem, we created an assistive navigation system that uses YOLO for object identification, BLIP for scene captioning, and an API for a generative AI model that provides precise descriptions and guidance of hazards in their path in real-time. Our model achieves a 72 percent success rate with guiding against hazards.
            </p>
            
        </section>

        <section id="teaser">
            <h2>Teaser figure</h2>
            Assisting Navigation and Hazards using vision on the smartphone
            <br><br>
            <div class="image-container">
                <img class="teaser-img" alt="Teaser Figure" src="teaser.png">
            </div>
            <p class="image-note">*This is not an image from the actual model*</p>
        </section>

        <section id = "introduction">
            <h2>Introduction</h2>
            <p>What if someone who is visually challenged could "see" what is going on around them? For the visually impaired, navigating the world may be a challenging task. While current solutions such as wearable technology, guide dogs, and specialized walking aids provide invaluable support, their accessibility, affordability, and efficacy are sometimes limited. On the other hand, depending just on a mobile phone offers a simpler and more economical method, taking advantage of smartphones' popularity.</p>

            <p>We are using a pre-trained YOLO model, which can perform real-time object detection with high accuracy. The system can efficiently identify hazards and obstacles such as vehicles, staircases, and pedestrians. When objects are detected, the results are sent to a BLIP model, which processes the scene and generates descriptive captions. These captions are then passed on to a generative AI tool via an API call. The AI predicts the most efficient course of action for the user. The resulting directions are converted into speech, which guides the user safely along the path. The approach combines fields in object detection, scene captioning, and generative AI.</p>
            <p>The table below are the related works which inspired us to build our project</p>

            <table border="1">
                <thead>
                    <tr>
                        <th style="background-color: #333344;">Title of Paper</th>
                        <th style="background-color: #333344;">Solution Implemented</th>
                        <th style="background-color: #333344;">Year Released</th>
                    </tr>
                </thead>
                <tbody>
                    <tr style="background-color: #333344; text-align: justify;">
                        <td>Mobility Assistive Technology with Artificial Intelligence: Indoor Navigation assistance for the visually impaired using Arduino-based assistive Goggles <a href="#references">[1]</td>
                        <td>A YOLO model was used to identify objects with COCO datasets. 3D-printed goggles were designed with a camera and sensor attached to capture images and measure distance. However, the goggles cannot be moved much as the sensor faces an error, and the shape of the goggles will have to be altered according to facial structures.</td>
                        <td style="text-align: center;">2023</td>
                    </tr>
                    <tr style="background-color: #333344; text-align: justify;">
                        <td>Realtime Indoor Navigation System for Visually Impaired Person using Direct-based Navigation <a href="#references">[3]</td>
                        <td>Canny edge detection was used to identify edges in videos and images. However, only two out of five tests were able to detect edges. The model must be more robust as it cannot accurately detect edges.</td>
                        <td style="text-align: center;">2022</td>
                    </tr>
                    <tr style="background-color: #333344; text-align: justify;">
                        <td>Arduino based Smart Walking Cane for Visually Impaired People <a href="#references">[2]</td>
                        <td>It uses a walking cane design with a water sensor to detect water and has an mp3 player connected for audio sounds. Efficiently minimized the weight of the cane. It has a resistor-based light to help users in dark environments. Overall, it is valuable and helpful to a user that uses a walking cane.</td>
                        <td style="text-align: center;">2020</td>
                    </tr>
                    <tr style="background-color: #333344; text-align: justify;">
                        <td>Camera Based Indoor Object Detection and Distance Estimation Framework for Assistive Mobility <a href="#references">[4]</td>
                        <td>It used the YOLOv7 framework for object detection, the Google Text to Speech Python library, and image depth perception. Very helpful for image detection.</td>
                        <td style="text-align: center;">2022</td>
                    </tr>
                    <tr style="background-color: #333344; text-align: justify;">
                        <td>Smart Assistive Navigator for the Blind using Image Processing <a href="#references">[5]</td>
                        <td>It uses the YOLO algorithm for object detection. Raspberry Pi is used to help locate edges. A webcam is used for high-quality video. But this model can only detect 20 objects.</td>
                        <td style="text-align: center;">2023</td>
                    </tr>
                </tbody>
            </table>
            
            <p>Compared to the above methods, this system stands out for the possible use of a widely available mobile device, which reduces the need for specialized tools. Additionally, the system offers a more adaptive and user-centric solution by incorporating generative AI to refine the directions.
            </p>

            <!-- add an image that shows workflow here or in the appraoch section -->

        </section>

        <section id = "approach">
            <h2>Approach</h2>
            <p>In order to be able to provide the required real-time hazard detection and guidance to visually impaired pedestrians, this project is going to rely on sophisticated Computer Vision and natural language processing techniques.</p>
            <p>To approach the problem, we broke it down into smaller tasks.</p>
            <ol>
                <li>Find an efficient way to detect objects in real time.</li>
                <li>Caption the scene with high accuracy.</li>
                <li>Utilize an LLM that can provide the best results for the scene in front of the user.</li>
                <li>Convert the instructions into audio-based instruction.</li>
            </ol>
            <p>Initially, we reviewed papers that could give us insight into the different methods and ideas used to solve the problem. We realized that no paper considered using a mobile device for object detection in this area, which inspired us to use the idea.</p>

            <!-- (mention some papers here or make a related works page) -->

            <p>The closest approach to our paper is <a href="#references">[1]</a>, which used 3D-printed glasses to detect objects using IoT.</p> 
            <!-- mention that paper -->

            <p><b>Task 1: Finding an object detection model</b></p>
            <p>After reviewing the papers, we realized that YOLO models are efficient in object recognition and can be easily trained. However, the YOLOv8 nano is a pre-trained model that can easily detect and classify objects in real time. We tested this model by sending it test videos of Blacksburg, where it quickly identified common hazards such as cars, pedestrians, and even benches.</p>
            <p>One problem we faced here is the ethical problem of having a pedestrian being recorded. We had to request permission from pedestrians to use the record data. Therefore, we could not test with multiple videos that involved pedestrians. However, even with limited testing, the model could accurately identify objects.</p>

            <p><b>Task 2: Captioning the objects with high accuracy.</b></p>
            <p>In the image or scene captioning world, many models accurately caption images in real-time. We experimented with the BLIP model, which captioned the objects sent by YOLO with almost complete accuracy. An observation noticed here was that sometimes the captions generated in an indoor environment were not as accurate compared to an outdoor environment. However, our model solely focuses on an outdoor environment, so this did not affect us.</p>

            <p><b>Task 3: Utilize an LLM that can provide the best results for the scene in front of the user.</b></p>
            <p>Our initial approach was to use a local transformer like TinyBert LLM, which can locally give directions. However, upon implementation, we realized that the model gave incorrect results and required a lot of training, which could take a lot of computational power. Overall, implementing such a model locally could potentially risk draining the phone's battery or overheating the device. We eventually decided to make an API call to a preexisting LLM online i.e ChatGPT 4. We gave a prompt to this model so it could identify solutions with ease. Upon testing with this LLM using live footage, we received accurate results in real time.</p>

            <p><b>Task 4: Convert the instructions into audio-based instruction.</b></p>
            <p>We first appraoched the problem by using a machine learning based text-to-speech model like Coque TTS to get a human like voice, but it did not run on our hardware. We then used <b>pyttsx3</b> which worked better but did not sound human-like</p>
        </section>
        

        <!-- <section id="workflow">
            <h2>Workflow</h2>
            <ol>
                <li>Capture video from a phone's camera.</li>
                <li>Process frames using YOLO or MobileNet SSD to detect objects.</li>
                <li>Use BLIP to generate captions describing detected objects.</li>
                <li>Assisting Directions: We will make use of TinyBERT LLM, specially optimized to generate helpful directions with a minimum amount of information for real time communication with the user.</li>
                <li> Dataset: Our dataset will consist of annotated images relevant for pedestrian navigation-objects and environmental hazards that may pose a danger to people commuting on foot.</li>
            </ol>
        </section> -->

        
        <section id="Experiments">
        <h2>Experiments and Results</h2>
        <p>We conducted a series of experiments for each task. As our idea has not been used previously, our tests were based on recorded videos and live footage.</p>
        <h3>Experimental Setup</h3>

        <p>For our experimental setup, we recorded videos around Blacksburg. The data collected for testing has been done by our team and not outsourced from any dataset. As we used pre-trained models that have trained with large datasets we did not require any training for our model. We inserted the videos onto our system and began experimenting for each task. Below is a table that demonstrates each method we used for different tasks, the results we obtained, and the final model selected for our system.</p>
        
        
        <table border="1">
            <thead>
                <tr style="background-color: #222233; text-align: center;">
                    <th colspan="8">Workflow for each task</th>
                </tr>
                <tr>
                    <th colspan="2" style="background-color: #ce5b5b;">Object Identifier</th>
                    <th colspan="2" style="background-color: #ce5b5b;">Scene Captioning</th>
                    <th colspan="2" style="background-color: #ce5b5b;">LLM Instruction</th>
                    <th colspan="2" style="background-color: #ce5b5b;">Text-to-Speech</th>
                </tr>
                <tr>
                    <th style="background-color: #bba318;">Model</th>
                    <th style="background-color: #bba318;">Results</th>
                    <th style="background-color: #0d5c0d;">Model</th>
                    <th style="background-color: #0d5c0d;">Results</th>
                    <th style="background-color: #311e9b;">Model</th>
                    <th style="background-color: #311e9b;">Results</th>
                    <th style="background-color: #81075d;">Model</th>
                    <th style="background-color: #81075d;">Results</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td style="background-color: #bba318;">YOLO nano</td>
                    <td style="background-color: #bba318;">No errors, objects identified accurately.</td>
                    <td style="background-color: #0d5c0d;">BLIP (hardcoded directions)</td>
                    <td style="background-color: #0d5c0d;">Multiple mistakes with live video.</td>
                    <td style="background-color: #311e9b;">TinyBert LLM</td>
                    <td style="background-color: #311e9b;">Multiple inaccuracies with live video.</td>
                    <td style="background-color: #81075d;">Toque ML model</td>
                    <td style="background-color: #81075d;">Could not run on hardware.</td>
                </tr>
                <tr>
                    <td style="background-color: #1e1e2f;"></td>
                    <td style="background-color: #1e1e2f;"></td>
                    <td style="background-color: #0d5c0d;">BLIP (context-based)</td>
                    <td style="background-color: #0d5c0d;">Accuracy improved tremendously.</td>
                    <td style="background-color: #311e9b;">PHI-2 LLM</td>
                    <td style="background-color: #311e9b;">Requires too much computational power.</td>
                    <td style="background-color: #81075d;">pyttsx3</td>
                    <td style="background-color: #81075d;">Voice not natural, but works.</td>
                </tr>
                <tr>
                    <td style="background-color: #1e1e2f;"></td>
                    <td style="background-color: #1e1e2f;"></td>
                    <td style="background-color: #0d5c0d;"></td>
                    <td style="background-color: #0d5c0d;">False captioning for indoor environments, but doesn't affect our use case.</td>
                    <td style="background-color: #311e9b;">ChatGPT 4 API</td>
                    <td style="background-color: #311e9b;">Accurate with live video testing.</td>
                    <td style="background-color: #1e1e2f;"></td>
                    <td style="background-color: #1e1e2f;"></td>
                </tr>
            </tbody>
        </table>

        <h3>Object Detection Experimentation</h3>
        
        <video id="Video2" style="width: 1100px; height: auto; display: block; margin: 0 auto;">
            <source src="results_video\Object_det_example.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
        
        <script>
            const video2 = document.getElementById("Video2");
        
            video2.addEventListener("click", () => {
            video2.play();
            });
        </script>
        <p style="text-align: center;">Video experimentation of the YOLO nano model</p>

        <h3>LLM Guidance experimentation</h3>
        <img src="results_video\guidance_image.png" alt="Image of the Guidance system working" style="display: block; margin: 0 auto; width: 1000px;">
        <p style="text-align: center;">Image of the Guidance system working using ChatGPT 4</p>
        <h3>Hazard Detection Accuracy Chart</h3>
        <div style="text-align: center;">
            <img src="./test_metrics.png" alt="Hazard Detection Accuracy Chart" style="width: 800px; height: auto;">
        </div>
        <p style="text-align: center;">Figure: Hazard detection accuracy for various test videos.</p>
        </section>


        <section id="qualitative-comparison">
            <h2>Qualitative Results</h2>

            <h3>Comparison with Baseline</h3>
            <div id="comparison-buttons" class="button-bar">
                <button onclick="showComparison('excellent')">Where Our Approach Exceeds Baseline</button>
                <button onclick="showComparison('failure')">Failure Case</button>
                <button onclick="showComparison('handlesWell')">Handles Well</button>
            </div>
        
            <div id="comparison-content" class="comparison-block">
                <!-- Excellent Case -->
                <div id="excellent" class="comparison-section">
                    <div class="video-container">
                        <video controls style="width: 500px; height: auto;">
                            <source src="results_video/excelent-case.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div class="text-container">
                        <h3>Where Our Approach Exceeds Baseline</h3>
                        <p>In this example, our approach successfully navigates a staircase scene using YOLO for object detection along with scene captioning and LLM guidance. The baseline approach would have missed the staircase and detected only benches, which could have resulted in incorrect guidance.</p>
                        <p>Our model gives instructions such as:</p>
                        <ul>
                            <li>Proceed forward carefully. Step up onto the staircase.</li>
                            <li>Use the handrail for support as you ascend the stairs.</li>
                        </ul>
                    </div>
                </div>
        
                <!-- Failure Case -->
                <div id="failure" class="comparison-section" style="display: none;">
                    <div class="video-container">
                        <video controls style="width: 500px; height: auto;">
                            <source src="results_video/bad-case.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div class="text-container">
                        <h3>Failure Case</h3>
                        <p>In this scene, the model fails to guide correctly. The LLM incorrectly identifies the environment as a store without detecting any immediate hazards. This indicates a gap in contextual recognition between object detection and scene captioning.</p>
                    </div>
                </div>
        
                <!-- Handles Well Case -->
                <div id="handlesWell" class="comparison-section" style="display: none;">
                    <div class="video-container">
                        <video controls style="width: 500px; height: auto;">
                            <source src="results_video/well-case.mp4" type="video/mp4">
                            Your browser does not support the video tag.
                        </video>
                    </div>
                    <div class="text-container">
                        <h3>Handles Well</h3>
                        <p>In this scene, the model effectively navigates through various hazards, providing timely and accurate guidance. The user is clearly guided to avoid obstacles, ensuring a safer experience.</p>
                    </div>
                </div>
            </div>
          
        </section>
        


        <!-- <section id = "qualitative_results">
            <h2>Qualitative Results</h2>
            <h3>Failure Case</h3>

            <video id="myVideo" style="width: 1100px; height: auto; display: block; margin: 0 auto;">
                <source src="results_video\Failure case.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>

            <script>
                const video = document.getElementById("myVideo");

                video.addEventListener("click", () => {
                video.play();
                });
            </script>

            <p> In this video a failure case example is shown. This is a supermarket environment where the user is navigating his way through. However the instructions mentioned at the top of the video states that there is a stop sign ahead, when it is not, and does not focus on the immediate hazard ahead which could result in a collision. The objects are also not detected accurately as there are multiple objects to detect and the user is moving in a faster pace.</p>
        </section> -->

        <section id = "conclusion">
            <h2>Conclusion</h2>
            <!-- Conclusion. Briefly summarize the report. “This report has described ….” Discuss any ideas that 
        you have to make your approach better.  -->
        <p>This report has described a novel way to assist the visually impaired on the road. It includes a detailed description of the different approaches used to solve the problem and uses object detection, object captioning, LLMs, and text-to-speech models to achieve this goal.</p>
            <p>With time, our approach can improve with the help of better lightweight LLMs, which will remove the dependency of an API call. We can also find or develop a more human-sounding text-to-speech model that could improve user comfort. Lastly, we can improve the depth perception of the model by training with more live data. </p>
        <h3>Disclaimer</h3>
    <p style="text-align: justify; color: #cccccc;">
        The demonstration videos presented in this project were constructed by stitching frames together, rather than capturing a continuous, real-time feed. Additionally, the guidance audio was generated using Text-to-Speech (TTS) and then manually spliced into the video. This approach was necessary because of the delay introduced by the API calls during real-time processing, which caused the video to pause and could not maintain a continuous visual and audio flow. The demonstration serves to show the accuracy and feasibility of the approach, though it may differ slightly from an entirely real-time experience.
    </p><h3>Disclaimer</h3>
    <p style="text-align: justify; color: #cccccc;">
        The demonstration videos presented in this project were constructed by stitching frames together, rather than capturing a continuous, real-time feed. Additionally, the guidance audio was generated using Text-to-Speech (TTS) and then manually spliced into the video. This approach was necessary because of the delay introduced by the API calls during real-time processing, which caused the video to pause and could not maintain a continuous visual and audio flow. The demonstration serves to show the accuracy and feasibility of the approach, though it may differ slightly from an entirely real-time experience.
    </p>
        </section>

        <section id = "references">
            <h2>References</h2>
            <ol>
                <li><a href="https://doi.org/10.1109/HNICEM60674.2023.10589004" target="_blank">J. R. B. Guevara et al., "Mobility Assistive Technology with Artificial Intelligence: Indoor Navigation Assistance for the Visually Impaired Using Arduino-Based Assistive Goggles," 2023 IEEE 15th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management (HNICEM), Coron, Palawan, Philippines, 2023, pp. 1-6, doi: 10.1109/HNICEM60674.2023.10589004.</a></li>
                <li><a href="https://doi.org/10.1109/ICISC47916.2020.9171209" target="_blank">M. Bansal, S. Malik, M. Kumar and N. Meena, "Arduino based Smart Walking Cane for Visually Impaired People," 2020 Fourth International Conference on Inventive Systems and Control (ICISC), Coimbatore, India, 2020, pp. 462-465, doi: 10.1109/ICISC47916.2020.9171209. keywords: {Legged locomotion;GSM;Radio frequency;Headphones;Blindness;Pressing;Feature extraction;Obstacle detection;Darkness detection;Water detection;GPS;GSM;RF remote},</a></li>
                <li><a href="https://doi.org/10.1109/IC2IE56416.2022.9970063" target="_blank">T. Mantoro and M. Zamzami, "Realtime Indoor Navigation System for Visually Impaired Person using Direct-based Navigation," 2022 5th International Conference of Computer and Informatics Engineering (IC2IE), Jakarta, Indonesia, 2022, pp. 320-324, doi: 10.1109/IC2IE56416.2022.9970063. keywords: {Economics;Cataracts;Technological innovation;Computer vision;Indoor navigation;Visual impairment;Machine learning;vision;blind;navigation;indoor;object detection}</a></li>
                <li><a href="https://doi.org/10.1109/SOLI57430.2022.10294458" target="_blank">V. K. Paswan and A. Choudhary, "Camera Based Indoor Object Detection and Distance Estimation Framework for Assistive Mobility," 2022 IEEE International Conference on Service Operations and Logistics, and Informatics (SOLI), Delhi, India, 2022, pp. 1-6, doi: 10.1109/SOLI57430.2022.10294458. keywords: {Deep learning;Navigation;Estimation;Object detection;Smart homes;Real-time systems;Indoor environment;smart home technology;Object detection;assistive mobility;visually impaired;deep learning;computer vision}</a></li>
                <li><a href="https://doi.org/10.1109/ICSCSS57650.2023.10169767" target="_blank">M. Kulkarni, M. Chitale, S. Chitpur, A. Chivate, P. Chopade and S. Deshmukh, "Smart Assistive Navigator for the Blind using Image Processing," 2023 International Conference on Sustainable Computing and Smart Systems (ICSCSS), Coimbatore, India, 2023, pp. 916-921, doi: 10.1109/ICSCSS57650.2023.10169767. keywords: {Deep learning;Navigation;Face recognition;Image processing;Object detection;Cameras;Real-time systems;Image Processing;Obstacle Detection;Internet of things;Assistive Aid;Blind;Raspberry Pi}</a></li>
                </ol>
        </section>
        <!-- <section id="demo">
            <h2>Demo Video</h2>
            <p>Check out a demo video of the system in action:</p>
            <video controls>
                <source src="demo.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </section> -->
    </main>
    <!-- <footer>
        <p style="text-align: center;"><b> © Project by Clifford Reeve Menezes and Pratheek Prakash Shetty</b></p>
        <p style="text-align: center;"><b>Group 39</b></p>
        <a href="https://github.com/gosLp/cv-assistive-nav" target="_blank">GitHub Repository</a>
    </footer> -->
    <footer>
        <p><strong>© Project by Clifford Reeve Menezes and Pratheek Prakash Shetty</strong></p>
        <p><strong>Group 39</strong></p>
        <a href="https://github.com/gosLp/cv-assistive-nav" target="_blank">GitHub Repository</a>
    </footer>
    <script>
        function showComparison(sectionId) {
            // Hide all sections
            const sections = document.querySelectorAll('.comparison-section');
            sections.forEach(section => {
                section.style.display = 'none';
            });
    
            // Show the selected section
            const selectedSection = document.getElementById(sectionId);
            if (selectedSection) {
                selectedSection.style.display = 'flex';
            }
        }
    </script>
    
    <script>
        // Smooth scroll behavior for the navbar
        document.querySelectorAll('.navbar a').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                document.querySelector(this.getAttribute('href')).scrollIntoView({
                    behavior: 'smooth'
                });
            });
        });
    </script>


</body>
</html>
