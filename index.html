<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Computer Vision Course Project
        | ECE, Virginia Tech | Fall 2024: ECE 4554/5554</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Vision based Assistive Navigation</h1>
        <span style="font-size: 18px; line-height: 1.5em;"><strong>Clifford Reeve Menezes, Pratheek Prakash Shetty</strong></span><br>
        <span style="font-size: 16px; line-height: 1.5em;">Fall 2024 ECE 4554/5554 Computer Vision: Course Project</span><br>
        <span style="font-size: 16px; line-height: 1.5em;">Virginia Tech</span>
        <p style="text-align: center;">This project uses object detection and image captioning models to provide hazard avoidance directions for users with visual impairments.</p>
    </header>

    <section id="abstract">
        <h2>Abstract</h2>
        <p>For visually impaired people, traveling alone in the modern world can be difficult, especially when crossing roadways or hazards in real-time. Our idea is to create safe smartphone navigation, eliminating the need for further special equipment while guaranteeing smooth guidance. Our aim us to help people having such problems and support them in their day-to-day activities. To solve this problem, we created an assistive navigation system that uses YOLO for object identification, BLIP for scene captioning, and an API for a generative AI model that provides precise descriptions and guidance of hazards in their path in real-time. Our model achieves a —--- success rate with —----something.
        </p>
        
    </section>
    <section id="teaser">
        <h2>Teaser figure</h2>
        Assisting Navigation and Hazards using vision on the smartphone
        <br><br>
        <div style="text-align: center;">
            <img style="height: 200px;" alt="" src="teaser.png">
        </div>
        <p style="text-align: center;">*This is not an image from the actual model*</p>
    </section>

    <section id = "introduction">
        <h2>Introduction</h2>
        <p>What if someone who is visually challenged could "see" what is going on around them? For the visually impaired, navigating the world may be a challenging task. While current solutions such as wearable technology, guide dogs, and specialized walking aids provide invaluable support, their accessibility, affordability, and efficacy are sometimes limited. On the other hand, depending just on a mobile phone offers a simpler and more economical method, taking advantage of smartphones' popularity.</p>

        <p>We are using a pre-trained YOLO model, which can perform real-time object detection with high accuracy. The system can efficiently identify hazards and obstacles such as vehicles, staircases, and pedestrians. When objects are detected, the results are sent to a BLIP model, which processes the scene and generates descriptive captions. These captions are then passed on to a generative AI tool via an API call. The AI predicts the most efficient course of action for the user. The resulting directions are converted into speech, which guides the user safely along the path. The approach combines fields in object detection, scene captioning, and generative AI.</p>

        <p>Compared to traditional methods, this system stands out for its use of a widely available mobile device, which reduces the need for specialized tools. Additionally, the system offers a more adaptive and user-centric solution by incorporating generative AI to refine the directions.
        </p>

        <!-- add an image that shows workflow here or in the appraoch section -->

    </section>

    <section id = "approach">
        <h2>Approach</h2>
        <p>In order to be able to provide the required real-time hazard detection and guidance to visually impaired pedestrians, this project is going to rely on sophisticated Computer Vision and natural language processing techniques.</p>
        <p>To approach the problem, we broke it down into smaller tasks.</p>
        <ol>
            <li>Find an efficient way to detect objects in real time.</li>
            <li>Caption the scene with high accuracy.</li>
            <li>Utilize an LLM that can provide the best results for the scene in front of the user.</li>
            <li>Convert the instructions into audio-based instruction.</li>
        </ol>
        <p>Initially, we reviewed papers that could give us insight into the different methods and ideas used to solve the problem. We realized that no paper considered using a mobile device for object detection in this area, which inspired us to use the idea.</p>

        <!-- (mention some papers here or make a related works page) -->

        <p>The closest approach to our paper is <a href="#references">[1]</a>, which used 3D-printed glasses to detect objects using IoT.</p> 
        <!-- mention that paper -->

        <p><b>Task 1: Finding an object detection model</b></p>
        <p>After reviewing the papers, we realized that YOLO models are efficient in object recognition and can be easily trained. However, the YOLOv8 nano is a pre-trained model that can easily detect and classify objects in real time. We tested this model by sending it test videos of Blacksburg, where it quickly identified common hazards such as cars, pedestrians, and even benches.</p>
        <p>One problem we faced here is the ethical problem of having a pedestrian being recorded. We had to request permission from pedestrians to use the record data. Therefore, we could not test with multiple videos that involved pedestrians. However, even with limited testing, the model could accurately identify objects.</p>

        <p><b>Task 2: Captioning the objects with high accuracy.</b></p>
        <p>In the image or scene captioning world, many models accurately caption images in real-time. We experimented with the BLIP model, which captioned the objects sent by YOLO with almost complete accuracy. An observation noticed here was that sometimes the captions generated in an indoor environment were not as accurate compared to an outdoor environment. However, our model solely focuses on an outdoor environment, so this did not affect us.</p>

        <p><b>Task 3: Utilize an LLM that can provide the best results for the scene in front of the user.</b></p>
        <p>Our initial approach was to use a local transformer like TinyBert LLM, which can locally give directions. However, upon implementation, we realized that the model gave incorrect results and required a lot of training, which could take a lot of computational power. Overall, implementing such a model locally could potentially risk draining the phone's battery or overheating the device. We eventually decided to make an API call to a preexisting LLM online i.e ChatGPT 4. We gave a prompt to this model so it could identify solutions with ease. Upon testing with this LLM using live footage, we received accurate results in real time.</p>

        <p><b>Task 4: Convert the instructions into audio-based instruction.</b></p>
        <p>We first appraoched the problem by using a machine learning based text-to-speech model like Coque TTS to get a human like voice, but it did not run on our hardware. We then used <b>pyttsx3</b> which worked better but did not sound human-like</p>
    </section>
    

    <!-- <section id="workflow">
        <h2>Workflow</h2>
        <ol>
            <li>Capture video from a phone's camera.</li>
            <li>Process frames using YOLO or MobileNet SSD to detect objects.</li>
            <li>Use BLIP to generate captions describing detected objects.</li>
            <li>Assisting Directions: We will make use of TinyBERT LLM, specially optimized to generate helpful directions with a minimum amount of information for real time communication with the user.</li>
            <li> Dataset: Our dataset will consist of annotated images relevant for pedestrian navigation-objects and environmental hazards that may pose a danger to people commuting on foot.</li>
        </ol>
    </section> -->

    
    <section id="Experiments">
    <h2>Experiments and Results</h2>

    <p>We conducted a series of experiments for each task. As our idea has not been used previously, our tests were based on recorded videos and live footage. Below is a table that gives an understanding of our experiments and findings.</p>
    
    <table border="1">
        <thead>
            <tr style="background-color: #222233; text-align: center;">
                <th colspan="8">Workflow for each task</th>
            </tr>
            <tr>
                <th colspan="2" style="background-color: #ce5b5b;">Object Identifier</th>
                <th colspan="2" style="background-color: #ce5b5b;">Scene Captioning</th>
                <th colspan="2" style="background-color: #ce5b5b;">LLM Instruction</th>
                <th colspan="2" style="background-color: #ce5b5b;">Text-to-Speech</th>
            </tr>
            <tr>
                <th style="background-color: #bba318;">Model</th>
                <th style="background-color: #bba318;">Results</th>
                <th style="background-color: #0d5c0d;">Model</th>
                <th style="background-color: #0d5c0d;">Results</th>
                <th style="background-color: #311e9b;">Model</th>
                <th style="background-color: #311e9b;">Results</th>
                <th style="background-color: #81075d;">Model</th>
                <th style="background-color: #81075d;">Results</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td style="background-color: #bba318;">YOLO nano</td>
                <td style="background-color: #bba318;">No errors, objects identified accurately.</td>
                <td style="background-color: #0d5c0d;">BLIP (hardcoded directions)</td>
                <td style="background-color: #0d5c0d;">Multiple mistakes with live video.</td>
                <td style="background-color: #311e9b;">TinyBert LLM</td>
                <td style="background-color: #311e9b;">Multiple inaccuracies with live video.</td>
                <td style="background-color: #81075d;">Toque ML model</td>
                <td style="background-color: #81075d;">Could not run on hardware.</td>
            </tr>
            <tr>
                <td style="background-color: #1e1e2f;"></td>
                <td style="background-color: #1e1e2f;"></td>
                <td style="background-color: #0d5c0d;">BLIP (context-based)</td>
                <td style="background-color: #0d5c0d;">Accuracy improved tremendously.</td>
                <td style="background-color: #311e9b;">PHI-2 LLM</td>
                <td style="background-color: #311e9b;">Requires too much computational power.</td>
                <td style="background-color: #81075d;">pyttsx3</td>
                <td style="background-color: #81075d;">Voice not natural, but works.</td>
            </tr>
            <tr>
                <td style="background-color: #1e1e2f;"></td>
                <td style="background-color: #1e1e2f;"></td>
                <td style="background-color: #0d5c0d;"></td>
                <td style="background-color: #0d5c0d;">False captioning for indoor environments, but doesn't affect our use case.</td>
                <td style="background-color: #311e9b;">ChatGPT 4 API</td>
                <td style="background-color: #311e9b;">Accurate with live video testing.</td>
                <td style="background-color: #1e1e2f;"></td>
                <td style="background-color: #1e1e2f;"></td>
            </tr>
        </tbody>
    </table>
</section>

<section id = "conclusion">
    <h2>Conclusion</h2>
    <!-- Conclusion. Briefly summarize the report. “This report has described ….” Discuss any ideas that 
you have to make your approach better.  -->
<p>This report has described a novel way to assist the visually impaired on the road. It includes a detailed description of the different approaches used to solve the problem and uses object detection, object captioning, LLMs, and text-to-speech models to achieve this goal.</p>
    <p>With time, our approach can improve with the help of better lightweight LLMs, which will remove the dependency of an API call. We can also find or develop a more human-sounding text-to-speech model that could improve user comfort. Lastly, we can improve the depth perception of the model by training with more live data. </p>
</section>

<section id = "references">
    <h2>References</h2>
    <ol>
        <li><a href="https://doi.org/10.1109/HNICEM60674.2023.10589004">J. R. B. Guevara et al., "Mobility Assistive Technology with Artificial Intelligence: Indoor Navigation Assistance for the Visually Impaired Using Arduino-Based Assistive Goggles," 2023 IEEE 15th International Conference on Humanoid, Nanotechnology, Information Technology, Communication and Control, Environment, and Management (HNICEM), Coron, Palawan, Philippines, 2023, pp. 1-6, doi: 10.1109/HNICEM60674.2023.10589004.</a></li>
        <li><a href="https://doi.org/10.1109/ICISC47916.2020.9171209">M. Bansal, S. Malik, M. Kumar and N. Meena, "Arduino based Smart Walking Cane for Visually Impaired People," 2020 Fourth International Conference on Inventive Systems and Control (ICISC), Coimbatore, India, 2020, pp. 462-465, doi: 10.1109/ICISC47916.2020.9171209. keywords: {Legged locomotion;GSM;Radio frequency;Headphones;Blindness;Pressing;Feature extraction;Obstacle detection;Darkness detection;Water detection;GPS;GSM;RF remote},</a></li>
        <li><a href="https://doi.org/10.1109/IC2IE56416.2022.9970063">T. Mantoro and M. Zamzami, "Realtime Indoor Navigation System for Visually Impaired Person using Direct-based Navigation," 2022 5th International Conference of Computer and Informatics Engineering (IC2IE), Jakarta, Indonesia, 2022, pp. 320-324, doi: 10.1109/IC2IE56416.2022.9970063. keywords: {Economics;Cataracts;Technological innovation;Computer vision;Indoor navigation;Visual impairment;Machine learning;vision;blind;navigation;indoor;object detection}</a></li>
        <li><a href="https://doi.org/10.1109/SOLI57430.2022.10294458">V. K. Paswan and A. Choudhary, "Camera Based Indoor Object Detection and Distance Estimation Framework for Assistive Mobility," 2022 IEEE International Conference on Service Operations and Logistics, and Informatics (SOLI), Delhi, India, 2022, pp. 1-6, doi: 10.1109/SOLI57430.2022.10294458. keywords: {Deep learning;Navigation;Estimation;Object detection;Smart homes;Real-time systems;Indoor environment;smart home technology;Object detection;assistive mobility;visually impaired;deep learning;computer vision}</a></li>
        <li><a href="https://doi.org/10.1109/ICSCSS57650.2023.10169767">M. Kulkarni, M. Chitale, S. Chitpur, A. Chivate, P. Chopade and S. Deshmukh, "Smart Assistive Navigator for the Blind using Image Processing," 2023 International Conference on Sustainable Computing and Smart Systems (ICSCSS), Coimbatore, India, 2023, pp. 916-921, doi: 10.1109/ICSCSS57650.2023.10169767. keywords: {Deep learning;Navigation;Face recognition;Image processing;Object detection;Cameras;Real-time systems;Image Processing;Obstacle Detection;Internet of things;Assistive Aid;Blind;Raspberry Pi}</a></li>
        </ol>
</section>
    <!-- <section id="demo">
        <h2>Demo Video</h2>
        <p>Check out a demo video of the system in action:</p>
        <video controls>
            <source src="demo.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </section> -->

    <footer>
        <p style="text-align: center;"><b> © Project by Clifford Reeve Menezes and Pratheek Prakash Shetty</b></p>
        <p style="text-align: center;"><b>Group 39</b></p>
        <a href="https://github.com/gosLp/cv-assistive-nav" target="_blank">GitHub Repository</a>
    </footer>
</body>
</html>
